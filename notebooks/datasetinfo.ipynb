{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d7eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Total .h5 files: 973\n",
      "‚úÖ Files with targets: 973\n",
      "‚ùå Files without targets: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def scan_and_count_targets(folder_path, target_keys=(\"reconstruction_rss\", \"reconstruction_esc\")):\n",
    "    \"\"\"\n",
    "    Scans all .h5 files in a folder and reports which ones contain target images.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str or Path): Path to the folder containing .h5 files.\n",
    "        target_keys (tuple): Possible keys indicating target images.\n",
    "\n",
    "    Returns:\n",
    "        dict: filename -> True/False depending on whether a target is found.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    count_with_target = 0\n",
    "    count_without_target = 0\n",
    "\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith(\".h5\"):\n",
    "            full_path = os.path.join(folder_path, fname)\n",
    "            try:\n",
    "                with h5py.File(full_path, 'r') as f:\n",
    "                    has_target = any(key in f for key in target_keys)\n",
    "                    results[fname] = has_target\n",
    "                    if has_target:\n",
    "                        count_with_target += 1\n",
    "                    else:\n",
    "                        count_without_target += 1\n",
    "            except Exception as e:\n",
    "                results[fname] = f\"Error: {e}\"\n",
    "\n",
    "    return results, count_with_target, count_without_target\n",
    "\n",
    "folder = r\"C:\\Users\\kostanjsek\\Documents\\knee_mri\\knee_singlecoil_train\\singlecoil_train\"\n",
    "results, has_target, no_target = scan_and_count_targets(folder)\n",
    "\n",
    "print(f\"üìÇ Total .h5 files: {len(results)}\")\n",
    "print(f\"‚úÖ Files with targets: {has_target}\")\n",
    "print(f\"‚ùå Files without targets: {no_target}\")\n",
    "\n",
    "#for fname, has_target in results.items():\n",
    "#    print(f\"{fname}: {'‚úÖ target present' if has_target is True else '‚ùå target missing' if has_target is False else has_target}\")\n",
    "\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a2baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists: True\n",
      "Files in folder:\n",
      "['file1000000.h5', 'file1000007.h5', 'file1000017.h5', 'file1000026.h5', 'file1000031.h5', 'file1000033.h5', 'file1000041.h5', 'file1000052.h5', 'file1000071.h5', 'file1000073.h5', 'file1000107.h5', 'file1000108.h5', 'file1000114.h5', 'file1000126.h5', 'file1000153.h5', 'file1000178.h5', 'file1000182.h5', 'file1000190.h5', 'file1000196.h5', 'file1000201.h5', 'file1000206.h5', 'file1000229.h5', 'file1000243.h5', 'file1000247.h5', 'file1000254.h5', 'file1000263.h5', 'file1000264.h5', 'file1000267.h5', 'file1000273.h5', 'file1000277.h5', 'file1000280.h5', 'file1000283.h5', 'file1000291.h5', 'file1000292.h5', 'file1000308.h5', 'file1000314.h5', 'file1000323.h5', 'file1000325.h5', 'file1000328.h5', 'file1000344.h5', 'file1000350.h5', 'file1000356.h5', 'file1000389.h5', 'file1000432.h5', 'file1000464.h5', 'file1000476.h5', 'file1000477.h5', 'file1000480.h5', 'file1000496.h5', 'file1000528.h5', 'file1000537.h5', 'file1000538.h5', 'file1000552.h5', 'file1000555.h5', 'file1000591.h5', 'file1000593.h5', 'file1000625.h5', 'file1000628.h5', 'file1000631.h5', 'file1000635.h5', 'file1000647.h5', 'file1000660.h5', 'file1000697.h5', 'file1000702.h5', 'file1000735.h5', 'file1000748.h5', 'file1000758.h5', 'file1000759.h5', 'file1000769.h5', 'file1000810.h5', 'file1000817.h5', 'file1000818.h5', 'file1000831.h5', 'file1000842.h5', 'file1000858.h5', 'file1000871.h5', 'file1000885.h5', 'file1000891.h5', 'file1000899.h5', 'file1000903.h5', 'file1000925.h5', 'file1000926.h5', 'file1000932.h5', 'file1000942.h5', 'file1000972.h5', 'file1000976.h5', 'file1000990.h5', 'file1001031.h5', 'file1001057.h5', 'file1001059.h5', 'file1001064.h5', 'file1001077.h5', 'file1001090.h5', 'file1001096.h5', 'file1001104.h5', 'file1001119.h5', 'file1001122.h5', 'file1001126.h5', 'file1001140.h5', 'file1001143.h5', 'file1001144.h5', 'file1001148.h5', 'file1001159.h5', 'file1001163.h5', 'file1001168.h5', 'file1001170.h5', 'file1001184.h5', 'file1001188.h5', 'file1001191.h5', 'file1001202.h5', 'file1001219.h5', 'file1001221.h5', 'file1001262.h5', 'file1001275.h5', 'file1001289.h5', 'file1001298.h5', 'file1001331.h5', 'file1001338.h5', 'file1001339.h5', 'file1001344.h5', 'file1001365.h5', 'file1001381.h5', 'file1001429.h5', 'file1001440.h5', 'file1001444.h5', 'file1001450.h5', 'file1001458.h5', 'file1001480.h5', 'file1001497.h5', 'file1001499.h5', 'file1001506.h5', 'file1001533.h5', 'file1001557.h5', 'file1001566.h5', 'file1001585.h5', 'file1001598.h5', 'file1001643.h5', 'file1001650.h5', 'file1001651.h5', 'file1001655.h5', 'file1001668.h5', 'file1001687.h5', 'file1001689.h5', 'file1001703.h5', 'file1001715.h5', 'file1001726.h5', 'file1001759.h5', 'file1001763.h5', 'file1001793.h5', 'file1001798.h5', 'file1001818.h5', 'file1001825.h5', 'file1001834.h5', 'file1001843.h5', 'file1001850.h5', 'file1001851.h5', 'file1001862.h5', 'file1001893.h5', 'file1001916.h5', 'file1001930.h5', 'file1001938.h5', 'file1001955.h5', 'file1001959.h5', 'file1001968.h5', 'file1001977.h5', 'file1001983.h5', 'file1001984.h5', 'file1001995.h5', 'file1001997.h5', 'file1002002.h5', 'file1002007.h5', 'file1002021.h5', 'file1002035.h5', 'file1002067.h5', 'file1002145.h5', 'file1002155.h5', 'file1002159.h5', 'file1002187.h5', 'file1002214.h5', 'file1002252.h5', 'file1002257.h5', 'file1002274.h5', 'file1002280.h5', 'file1002340.h5', 'file1002351.h5', 'file1002377.h5', 'file1002380.h5', 'file1002382.h5', 'file1002389.h5', 'file1002404.h5', 'file1002412.h5', 'file1002417.h5', 'file1002436.h5', 'file1002451.h5', 'file1002515.h5', 'file1002526.h5', 'file1002538.h5', 'file1002546.h5', 'file1002570.h5']\n",
      "Found 199 .h5 files:\n",
      "Keys inside file:\n",
      "['ismrmrd_header', 'kspace', 'reconstruction_esc', 'reconstruction_rss']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "folder = r\"C:\\Users\\kostanjsek\\Documents\\knee_mri\\knee_singlecoil_val\\singlecoil_val\"  # or whichever folder you're testing\n",
    "print(\"Folder exists:\", os.path.exists(folder))\n",
    "print(\"Files in folder:\")\n",
    "print(os.listdir(folder))\n",
    "\n",
    "h5_files = [f for f in os.listdir(folder) if f.endswith(\".h5\")]\n",
    "print(f\"Found {len(h5_files)} .h5 files:\")\n",
    "\n",
    "\n",
    "test_file = os.path.join(folder, h5_files[0])  # first file\n",
    "with h5py.File(test_file, 'r') as f:\n",
    "    print(\"Keys inside file:\")\n",
    "    print(list(f.keys()))\n",
    "    print(len(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b308c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slices: 7135\n",
      "Top 5 files by slice count:\n",
      "file1002351.h5: 46 slices\n",
      "file1002451.h5: 46 slices\n",
      "file1001763.h5: 45 slices\n",
      "file1001938.h5: 45 slices\n",
      "file1000277.h5: 42 slices\n",
      "File: file1002570.h5\n",
      "Number of slices: 33\n"
     ]
    }
   ],
   "source": [
    "def count_slices_per_file(folder_path):\n",
    "    slice_counts = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith(\".h5\"):\n",
    "            path = os.path.join(folder_path, fname)\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                num_slices = f['kspace'].shape[0]\n",
    "                slice_counts[fname] = num_slices\n",
    "    return slice_counts\n",
    "\n",
    "# Example usage:\n",
    "folder = r\"C:\\Users\\kostanjsek\\Documents\\knee_mri\\knee_singlecoil_val\\singlecoil_val\"\n",
    "counts = count_slices_per_file(folder)\n",
    "print(f\"Total slices: {sum(counts.values())}\")\n",
    "print(\"Top 5 files by slice count:\")\n",
    "for k in sorted(counts, key=counts.get, reverse=True)[:5]:\n",
    "    print(f\"{k}: {counts[k]} slices\")\n",
    "\n",
    "\n",
    "def count_slices_single_file(folder_path):\n",
    "    # Get a sorted list of .h5 files\n",
    "    h5_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".h5\")])\n",
    "    if not h5_files:\n",
    "        print(\"No .h5 files found in folder.\")\n",
    "        return\n",
    "\n",
    "    single_file = h5_files[198]\n",
    "    single_file_path = os.path.join(folder_path, single_file)\n",
    "\n",
    "    with h5py.File(single_file_path, 'r') as f:\n",
    "        num_slices = f['kspace'].shape[0]\n",
    "        print(f\"File: {single_file}\")\n",
    "        print(f\"Number of slices: {num_slices}\")    \n",
    "\n",
    "count_slices_single_file(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c3181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['ismrmrd_header', 'kspace', 'reconstruction_esc', 'reconstruction_rss']\n",
      "Attrs: {'acquisition': 'CORPDFS_FBK', 'max': np.float64(0.0001537275713242826), 'norm': np.float64(0.04750838076305816), 'patient_id': 'd7f12ba7cfb097ea9d97c2896be202db152288abcb76335dd4ec82b02fd759d8'}\n",
      "complex64\n",
      "(32, 640, 372)\n"
     ]
    }
   ],
   "source": [
    "import fastmri\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = r\"C:\\Users\\kostanjsek\\Documents\\knee_mri\\knee_singlecoil_val\\singlecoil_val\\file1000178.h5\"\n",
    "\n",
    "#with h5py.File(file_path, 'r') as f:\n",
    "#    print(f\"Keys in {file_path}: {list(f.keys())}\")\n",
    "\n",
    "hf = h5py.File(file_path)\n",
    "print('Keys:', list(hf.keys()))\n",
    "print('Attrs:', dict(hf.attrs))\n",
    "\n",
    "volume_kspace = hf['kspace'][()]\n",
    "print(volume_kspace.dtype)\n",
    "print(volume_kspace.shape) # (number of slices, height, width)\n",
    "\n",
    "slice_kspace = volume_kspace[20] # Choosing the 20-th slice of this volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11dc0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SliceDataset in module fastmri.data.mri_data:\n",
      "\n",
      "class SliceDataset(torch.utils.data.dataset.Dataset)\n",
      " |  SliceDataset(\n",
      " |      root: Union[str, pathlib._local.Path, os.PathLike],\n",
      " |      challenge: str,\n",
      " |      transform: Optional[Callable] = None,\n",
      " |      use_dataset_cache: bool = False,\n",
      " |      sample_rate: Optional[float] = None,\n",
      " |      volume_sample_rate: Optional[float] = None,\n",
      " |      dataset_cache_file: Union[str, pathlib._local.Path, os.PathLike] = 'dataset_cache.pkl',\n",
      " |      num_cols: Optional[Tuple[int]] = None,\n",
      " |      raw_sample_filter: Optional[Callable] = None\n",
      " |  )\n",
      " |\n",
      " |  A PyTorch Dataset that provides access to MR image slices.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      SliceDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __getitem__(self, i: int)\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      root: Union[str, pathlib._local.Path, os.PathLike],\n",
      " |      challenge: str,\n",
      " |      transform: Optional[Callable] = None,\n",
      " |      use_dataset_cache: bool = False,\n",
      " |      sample_rate: Optional[float] = None,\n",
      " |      volume_sample_rate: Optional[float] = None,\n",
      " |      dataset_cache_file: Union[str, pathlib._local.Path, os.PathLike] = 'dataset_cache.pkl',\n",
      " |      num_cols: Optional[Tuple[int]] = None,\n",
      " |      raw_sample_filter: Optional[Callable] = None\n",
      " |  )\n",
      " |      Args:\n",
      " |          root: Path to the dataset.\n",
      " |          challenge: \"singlecoil\" or \"multicoil\" depending on which challenge\n",
      " |              to use.\n",
      " |          transform: Optional; A callable object that pre-processes the raw\n",
      " |              data into appropriate form. The transform function should take\n",
      " |              'kspace', 'target', 'attributes', 'filename', and 'slice' as\n",
      " |              inputs. 'target' may be null for test data.\n",
      " |          use_dataset_cache: Whether to cache dataset metadata. This is very\n",
      " |              useful for large datasets like the brain data.\n",
      " |          sample_rate: Optional; A float between 0 and 1. This controls what fraction\n",
      " |              of the slices should be loaded. Defaults to 1 if no value is given.\n",
      " |              When creating a sampled dataset either set sample_rate (sample by slices)\n",
      " |              or volume_sample_rate (sample by volumes) but not both.\n",
      " |          volume_sample_rate: Optional; A float between 0 and 1. This controls what fraction\n",
      " |              of the volumes should be loaded. Defaults to 1 if no value is given.\n",
      " |              When creating a sampled dataset either set sample_rate (sample by slices)\n",
      " |              or volume_sample_rate (sample by volumes) but not both.\n",
      " |          dataset_cache_file: Optional; A file in which to cache dataset\n",
      " |              information for faster load times.\n",
      " |          num_cols: Optional; If provided, only slices with the desired\n",
      " |              number of columns will be considered.\n",
      " |          raw_sample_filter: Optional; A callable object that takes an raw_sample\n",
      " |              metadata as input and returns a boolean indicating whether the\n",
      " |              raw_sample should be included in the dataset.\n",
      " |\n",
      " |  __len__(self)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |\n",
      " |  __add__(self, other: 'Dataset[_T_co]') -> 'ConcatDataset[_T_co]'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |\n",
      " |  __orig_bases__ = (typing.Generic[+_T_co],)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastmri.data import SliceDataset\n",
    "help(SliceDataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
